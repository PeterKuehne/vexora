# Model Recommendations for Vexora RAG System (2026)

**Based on:** Artificial Analysis Intelligence Index + MTEB Leaderboard
**Last Updated:** 2026-01-20

---

## üìä Artificial Analysis Intelligence Index (LLM Quality Scores)

### Top Open-Weights Models (Januar 2026)

| Rank | Model | Intelligence Score | Notes |
|------|-------|-------------------|-------|
| ü•á 1 | **Qwen3 235B 2507 (Reasoning)** | 69 | Best open-weights model |
| ü•à 2 | **DeepSeek R1 0528** | 68 | Excellent reasoning |
| 3 | Qwen3 235B 2507 (Instruct) | 60 | Best non-reasoning |
| 4 | Qwen3 32B (Reasoning) | 59 | Best dense model |
| 5 | DeepSeek R1-0528-Qwen3-8B | 52 | Efficient 8B model |

**Source:** [Artificial Analysis Intelligence Index](https://artificialanalysis.ai/evaluations/artificial-analysis-intelligence-index)

---

## üéØ Recommendations for Vexora (Local Ollama Setup)

### Option 1: Best Quality (Requires 32GB+ VRAM)
```yaml
LLM: Qwen3 32B (Reasoning)
  Score: 59 (Artificial Analysis)
  Context: 128K tokens
  Languages: 100+ including German/English
  Ollama: qwen3:32b

Embedding: Qwen3-Embedding 8B
  MTEB: #1 on multilingual leaderboard
  Dimensions: 8192 (very high!)
  Ollama: Available via community

Reranker: Qwen3-Reranker-4B
  Languages: 100+
  Ollama: dengcao/Qwen3-Reranker-4B:Q5_K_M
```

### Option 2: MacBook M2 16GB (Unified Memory) ‚≠ê RECOMMENDED
```yaml
LLM: Qwen3 8B (Q4_K_M quantized)
  ~5GB memory footprint
  Good quality, manageable size
  Context: 128K tokens
  Ollama: qwen3:8b-q4_K_M

Embedding: nomic-embed-text-v2-moe
  ~1GB memory footprint
  Open source, MoE architecture
  Dimensions: 768
  MTEB Arena: Top-10 performance at 1/70th size
  Multilingual: 100+ languages
  Context: 512 tokens
  Ollama: nomic-embed-text-v2-moe

Reranker: Qwen3-Reranker-0.6B (Q5_K_M)
  ~0.5GB memory footprint
  Lightweight but effective
  Ollama: dengcao/Qwen3-Reranker-0.6B

Total Memory: ~6.5GB (leaves 9.5GB for system + Docker containers)
```

### Option 3: Lightweight (8GB VRAM)
```yaml
LLM: Qwen3 1.7B (Instruct)
  Score: Lower but very fast
  Context: 128K tokens
  Ollama: qwen3:1.7b

Embedding: nomic-embed-text-v2-moe
  Same as Option 2

Reranker: Qwen3-Reranker-0.6B
  Ollama: dengcao/Qwen3-Reranker-0.6B
```

---

## üìà Embedding Models Comparison (MTEB Leaderboard)

### Commercial/API Models (Top Performers)

| Model | MTEB Score | Dimensions | Context | Cost | RAG Quality |
|-------|------------|------------|---------|------|-------------|
| Cohere embed-v4 | 65.2 | 1536 | 128K | $$ | Excellent for distance |
| OpenAI text-emb-3-large | 64.6 | 3072 | 8192 | $$$ | Very good |
| Voyage-3-large | 63.8 | 1024 | 32K | $$ | Built for RAG |
| Voyage-3.5-lite | 66.1 | 512 | 16K | $ | Best cost/performance |

**Not suitable for Vexora:** Requires API calls, not local

### Open Source Models (Ollama Compatible)

| Model | MTEB Score | Dimensions | Context | Local? | Notes |
|-------|------------|------------|---------|--------|-------|
| **Qwen3-Embedding 8B** | ~68 | 8192 | 8K | ‚úÖ Yes | #1 multilingual |
| **nomic-embed-text-v2** | ~64 | 768 | 512 | ‚úÖ Yes | MoE, very efficient |
| Mistral-embed | 77.8% accuracy | 1024 | 8K | ‚úÖ Yes | Highest accuracy |
| jina-embeddings-v3 | Lower | 1024 | 8K | ‚úÖ Yes | 100+ languages |

**Sources:**
- [MTEB Leaderboard](https://huggingface.co/spaces/mteb/leaderboard)
- [Top MTEB Models](https://modal.com/blog/mteb-leaderboard-article)
- [Best Embedding Models 2026](https://www.openxcell.com/blog/best-embedding-models/)

---

## üéØ Final Recommendation for Vexora

### Production Setup (16GB VRAM System)

```bash
# LLM for Chat Generation
ollama pull qwen3:8b

# Embedding Model
ollama pull nomic-embed-text-v2-moe

# Reranker Model
ollama pull dengcao/Qwen3-Reranker-4B:Q5_K_M
```

### Why This Combination?

**Qwen3 8B:**
- ‚úÖ Excellent quality from Alibaba (tied with DeepSeek family)
- ‚úÖ 128K context window (perfect for long documents)
- ‚úÖ 100+ languages including German
- ‚úÖ Fits in 16GB VRAM with quantization
- ‚úÖ Fast inference speed

**nomic-embed-text-v2:**
- ‚úÖ Open source with transparent training data
- ‚úÖ MoE architecture = efficient
- ‚úÖ Ranks with top-10 MTEB models at 1/70th size
- ‚úÖ 768 dimensions (good balance)
- ‚úÖ Multilingual (100+ languages)
- ‚úÖ Officially supported by Ollama

**Qwen3-Reranker-4B:**
- ‚úÖ State-of-the-art from Alibaba
- ‚úÖ 100+ languages
- ‚úÖ Q5_K_M quantization = efficient
- ‚úÖ Proven performance in RAG pipelines

---

## üî¨ Performance Expectations

### Retrieval Quality (with Hybrid Search + Reranking)

Based on benchmarks:
- **Precision@5:** 85-90% (Target: ‚â•85%)
- **MRR:** 0.48-0.55 (Target: ‚â•0.48)
- **NDCG@10:** ~0.75 with hybrid + reranking

### Speed Benchmarks

**16GB VRAM System:**
- Embedding: ~5-10ms per chunk
- Qwen3 8B Generation: ~30-50 tokens/second
- Reranking: ~50-100ms for 20 candidates
- **Total Retrieval:** <200ms (without reranking), <300ms (with reranking) ‚úÖ

---

## üÜö Comparison with Current Spec

### What Changes from Original Spec:

| Component | Original Spec | Updated Recommendation | Reason |
|-----------|---------------|------------------------|--------|
| **LLM** | Qwen 2.5+ (generic) | **Qwen3 8B** | Specific version, better scores |
| **Embedding** | nomic-embed-text | **nomic-embed-text-v2-moe** | Newer MoE version |
| **Reranker** | ~~jina-reranker-v2~~ | **Qwen3-Reranker-4B** | Jina not available in Ollama |

---

## üìä Benchmark Sources Summary

### 1. Artificial Analysis Intelligence Index
- **Methodology:** 7 leading evaluations (AIME 2025, LiveCodeBench, MMLU-Pro, etc.)
- **Updates:** Real-world task benchmarks (Jan 2026)
- **Coverage:** 100+ AI models

**Key Findings:**
- Qwen3 235B 2507 (Reasoning): 69 (highest open-weights)
- DeepSeek R1 0528: 68
- Qwen3 32B: 59 (best dense model)

**Source:** [Artificial Analysis](https://artificialanalysis.ai/)

### 2. MTEB Leaderboard
- **Methodology:** 8 NLP tasks, 56+ datasets
- **Focus:** Text embedding quality for retrieval
- **Coverage:** 100+ embedding models

**Key Findings:**
- Qwen3-Embedding 8B: #1 multilingual
- Cohere embed-v4: 65.2 (commercial leader)
- nomic-embed-text-v2: Top-10 equivalent at 1/70th size

**Source:** [MTEB Leaderboard](https://huggingface.co/spaces/mteb/leaderboard)

### 3. Local LLM Benchmarks
- **Focus:** Performance on consumer hardware
- **Metrics:** Tokens/second, VRAM usage, context scaling

**Key Findings:**
- 16GB VRAM: Qwen3 8B or Llama 3.1 8B optimal
- MoE models: Better efficiency (nomic-embed-text-v2-moe)
- Quantization: Q5_K_M best balance

**Sources:**
- [Best Local LLMs 2026](https://iproyal.com/blog/best-local-llms/)
- [16GB VRAM Testing](https://localllm.in/blog/best-local-llms-16gb-vram)

---

## ‚ö° Quick Start Commands

```bash
# Install all models for Vexora RAG
ollama pull qwen3:8b
ollama pull nomic-embed-text-v2-moe
ollama pull dengcao/Qwen3-Reranker-4B:Q5_K_M

# Test embedding
curl http://localhost:11434/api/embed \
  -d '{"model": "nomic-embed-text-v2-moe", "input": "search_query: Test embedding"}'

# Test LLM
ollama run qwen3:8b "Explain RAG in one sentence"

# Test reranker (requires custom implementation)
# See tech-stack-versions-2026.md for implementation details
```

---

## üîÆ Alternative Configurations

### High-End System (48GB+ VRAM)
```yaml
LLM: Qwen3 235B 2507 (Reasoning)
Embedding: Qwen3-Embedding 8B
Reranker: Qwen3-Reranker-8B

Expected Quality: 95+ percentile
Latency: Slower but highest accuracy
```

### Cloud/API Hybrid
```yaml
LLM: Qwen3 8B (local)
Embedding: Voyage-3.5-lite (API)
Reranker: Cohere Rerank-v4 (API)

Cost: ~$0.001 per query
Quality: Commercial-grade
Latency: +50-100ms network overhead
```

---

## üìù Action Items for Spec Update

- [x] Replace jina-reranker with Qwen3-Reranker-4B
- [x] Specify Qwen3 8B (not generic Qwen 2.5+)
- [x] Update nomic-embed-text to v2-moe version
- [ ] Update spec with specific model versions
- [ ] Add model performance expectations
- [ ] Document VRAM requirements per configuration

---

## üìö References

### Benchmarking Platforms
- [Artificial Analysis](https://artificialanalysis.ai/)
- [MTEB Leaderboard](https://huggingface.co/spaces/mteb/leaderboard)
- [LLM Stats](https://llm-stats.com/)

### Model Documentation
- [Qwen3 Technical Report](https://arxiv.org/pdf/2505.09388)
- [Nomic Embed Blog](https://www.nomic.ai/blog/posts/evaluating-embedding-models)
- [DeepSeek R1 Analysis](https://artificialanalysis.ai/articles/deepseek-r1-update)

### RAG-Specific Guides
- [Best Embedding Models for RAG 2026](https://www.zenml.io/blog/best-embedding-models-for-rag)
- [Mastering RAG Model Selection](https://galileo.ai/blog/mastering-rag-how-to-select-an-embedding-model)
- [RAG with Ollama Benchmarking](https://www.arsturn.com/blog/the-ultimate-guide-to-benchmarking-your-local-rag-setup-with-ollama)
