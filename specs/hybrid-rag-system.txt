PROJECT: Vexora Hybrid RAG System

================================================================================
CURRENT STATE
================================================================================

Vexora is an existing AI chat application with the following functionality:

- Users can have conversations with Ollama-powered LLMs (currently supports Qwen and other models)
- Real-time streaming responses from LLM
- Multiple conversation management (create, switch, delete conversations)
- Persistent conversation history (stored in browser localStorage)
- Ollama model selection and management
- Advanced LLM settings (temperature, top_p, top_k, tokens)
- Persistent system prompt configuration
- Light/Dark theme support
- Markdown rendering with code syntax highlighting
- Message editing and regeneration
- Auto-save functionality
- Settings import/export

TECH STACK:
- Frontend: React 19 + TypeScript + Vite
- Backend: Express.js + TypeScript
- LLM: Ollama (local)
- Storage: Browser localStorage only (no database)
- Styling: Tailwind CSS

DEPLOYMENT:
- Runs completely local
- No cloud services
- Single-user application

================================================================================
FEATURES TO IMPLEMENT
================================================================================

MAIN GOAL:
Add Retrieval-Augmented Generation (RAG) capabilities to Vexora so users can chat with their own documents and structured data.

---

REQUIREMENT 1: Document Upload and Indexing
Priority: HIGH

Users should be able to:
- Upload PDF documents through the UI
- See list of all uploaded documents
- View document processing status (pending, processing, completed, failed)
- Delete documents they no longer need
- Filter documents by category or tags
- Add metadata to documents (category, tags, custom fields)

Documents must be:
- Split into searchable chunks
- Converted to vector embeddings
- Indexed for fast retrieval
- Stored with metadata (filename, upload date, page numbers)

CONSTRAINTS:
- Maximum file size: 50MB per PDF
- Must handle multi-page PDFs
- Processing must not block the UI
- Users must see upload progress
- Failed uploads must show clear error messages

SUCCESS CRITERIA:
- User can upload a 20-page PDF and see it indexed within 30 seconds
- User can see processing progress in real-time
- User receives notification when processing completes or fails
- Uploaded documents persist across browser sessions

OUT OF SCOPE:
- Word documents, Excel files (only PDF for MVP)
- Batch upload of multiple files simultaneously
- OCR for scanned PDFs
- Document versioning

---

REQUIREMENT 2: Hybrid Search (Vector + Keyword)
Priority: HIGH

The system must retrieve relevant document chunks using:
- Semantic search (vector similarity)
- Keyword search (exact/fuzzy matching)
- Combined scoring (hybrid approach)

Users should be able to:
- Configure search behavior (more semantic vs more keyword-based)
- Filter search by document metadata
- See which documents were used in the response

CONSTRAINTS:
- Search latency must be under 300ms
- Must support German and English documents
- Must handle documents up to 1000 pages
- Minimum 1000 documents supported

SUCCESS CRITERIA:
- Finding "Datenschutz DSGVO Artikel 6" returns relevant chunks from privacy policy
- Searching for exact product codes works reliably
- Users can understand why certain documents were retrieved

LIBRARIES:
- Weaviate (vector database with built-in hybrid search)
- Ollama (for embedding models - configurable via settings)

OUT OF SCOPE:
- Multi-language search across more than 2 languages
- Image/diagram search within PDFs
- Real-time document updates

---

REQUIREMENT 3: Reranking for Better Accuracy
Priority: MEDIUM

After initial retrieval, the system must:
- Re-score candidate chunks for relevance
- Use cross-encoder model for accurate ranking
- Return only top 3-5 most relevant chunks

CONSTRAINTS:
- Reranking must complete within 100ms
- Must work offline (local model)
- Must support German and English

SUCCESS CRITERIA:
- Retrieval precision improves by at least 15% with reranking enabled
- Users can toggle reranking on/off in settings
- Response quality is noticeably better with reranking

LIBRARIES:
- Ollama (for reranker models - configurable via settings)

OUT OF SCOPE:
- Custom reranking model training
- A/B testing framework

---

REQUIREMENT 4: RAG-Enhanced Chat
Priority: HIGH

Users should be able to:
- Enable/disable RAG per conversation
- See source citations in LLM responses
- Click on citations to view original document/page
- Configure how many chunks to retrieve (top-K)
- Filter which documents to search (by category/tags)

The system must:
- Retrieve relevant chunks before generating response
- Include chunk context in LLM prompt
- Stream responses with citations
- Show which documents contributed to answer

CONSTRAINTS:
- RAG must not significantly slow down response time (max +500ms)
- Citations must be accurate (correct document and page number)
- RAG must work with existing streaming architecture
- Must gracefully handle no relevant documents found

SUCCESS CRITERIA:
- User asks "What does our privacy policy say about GDPR Article 6?" and gets accurate answer with source citation
- Citations link directly to the source document/page
- Response indicates "Based on 3 documents: privacy_policy.pdf (page 12), gdpr_guide.pdf (page 5)..."
- User can disable RAG and get normal chat behavior

OUT OF SCOPE:
- Multi-hop reasoning across documents
- Automatic follow-up questions
- Citation confidence scores

---

REQUIREMENT 5: Structured Data Queries
Priority: MEDIUM

Users should be able to:
- Query structured data from PostgreSQL database
- Combine document search with database queries
- Get answers that merge both sources

The system must:
- Store structured business data in PostgreSQL
- Support natural language to SQL conversion
- Combine database results with document retrieval
- Provide clear indication when database was queried

CONSTRAINTS:
- Read-only database access (no writes/updates)
- Must work with existing PostgreSQL schemas
- Queries must complete within 2 seconds
- Must prevent SQL injection

SUCCESS CRITERIA:
- User asks "How many customers do we have in Berlin?" and gets answer from database
- User asks "What does the manual say about customer onboarding?" and gets answer from documents
- System can handle questions requiring both sources

OUT OF SCOPE:
- Database schema modifications
- Complex multi-table joins
- Real-time database syncing
- Write operations to database

---

REQUIREMENT 6: Document Management UI
Priority: HIGH

Users need a dashboard to:
- View all uploaded documents
- See processing status and errors
- Search/filter documents
- Delete documents
- Preview document metadata
- Bulk delete multiple documents

CONSTRAINTS:
- Must be responsive (mobile-friendly)
- Must show real-time processing updates
- Must handle 1000+ documents without performance issues
- Must integrate seamlessly with existing UI theme

SUCCESS CRITERIA:
- User can find any document within 3 clicks
- User can see why a document failed to process
- User can delete multiple documents at once
- Dashboard loads in under 1 second even with 1000 documents

OUT OF SCOPE:
- Document preview/reader
- Collaborative document sharing
- Document annotations

---

REQUIREMENT 7: Model Configuration & Flexibility
Priority: HIGH

Users should be able to configure:
- Select LLM model from available Ollama models
- Select embedding model from available Ollama models
- Select reranker model from available Ollama models
- Switch between model profiles (e.g., "MacBook 16GB", "Workstation 64GB")
- Create custom model configurations
- View current model memory usage
- Test model configuration before applying

The system must:
- Auto-discover available Ollama models
- Detect embedding dimension changes
- Offer to re-index documents when embedding model changes
- Show model compatibility warnings
- Support profile presets for different hardware configurations

CONSTRAINTS:
- Must not hardcode model names in application code
- Must handle different embedding dimensions (768, 1024, 8192, etc.)
- Must work on MacBook M2 16GB unified memory
- Must support future hardware upgrades without code changes
- Settings must persist across sessions

SUCCESS CRITERIA:
- User can switch from 8B to 32B LLM model without code changes
- User can upgrade embedding model and system handles re-indexing
- System shows "Model incompatible with 16GB memory" warning appropriately
- Profile switching works seamlessly (select preset → apply → done)
- New models pulled via Ollama automatically appear in UI

OUT OF SCOPE:
- Cloud API providers (OpenAI, Anthropic) in MVP
- Custom model training
- Automatic model selection based on query complexity

---

REQUIREMENT 8: RAG Configuration & Settings
Priority: MEDIUM

Users should be able to configure:
- Hybrid search balance (semantic vs keyword weight: alpha parameter)
- Number of chunks to retrieve (top-K: 1-10)
- Enable/disable reranking
- Default categories/tags
- Chunk size and overlap (advanced setting)

CONSTRAINTS:
- Settings must persist across sessions
- Changes must apply immediately
- Must provide sensible defaults
- Must include helpful tooltips

SUCCESS CRITERIA:
- User can adjust alpha from 0.0 (pure keyword) to 1.0 (pure semantic)
- User can increase top-K to 10 for more context
- Settings UI is intuitive for non-technical users

OUT OF SCOPE:
- A/B testing of different configurations
- Automatic parameter tuning

---

REQUIREMENT 9: Analytics & Monitoring
Priority: LOW

Users should be able to see:
- Total number of documents indexed
- Storage usage
- Most frequently queried documents
- Average retrieval time
- Query success rate

CONSTRAINTS:
- Analytics must not impact performance
- Data stored locally (no external analytics)
- Basic visualizations only

SUCCESS CRITERIA:
- User can see total documents and storage used
- User can identify which documents are never used
- System logs errors for debugging

OUT OF SCOPE:
- Advanced analytics dashboard
- Query optimization recommendations
- Cost tracking

================================================================================
TECHNICAL STACK
================================================================================

NEW DEPENDENCIES:

Vector Database:
- Weaviate 1.34.8+ (native hybrid search, runs in Docker)

Structured Data:
- PostgreSQL 16+ with pgvector 0.8.1 extension

Cache & Queue:
- Redis 7.x (for caching and background job processing)

AI Models (via Ollama - User Configurable):
- LLM: Any Ollama model (e.g., qwen3:8b, llama3, mistral)
- Embedding: Any Ollama embedding model (e.g., nomic-embed-text, qwen3-embedding)
- Reranker: Any Ollama reranker model (e.g., qwen3-reranker series)
- NOTE: Models are NOT hardcoded - users can switch via settings
- NOTE: System provides preset profiles for different hardware configs

PDF Processing:
- unpdf (npm, modern TypeScript alternative to pdf-parse)
- NOTE: pdf-parse is unmaintained, use unpdf instead

Orchestration:
- LlamaIndex (TypeScript version, 0.12.0+)

RECOMMENDED STARTER CONFIGURATION (MacBook M2 16GB):
- LLM: qwen3:8b-q4_K_M (~5GB)
- Embedding: nomic-embed-text (~0.5GB, 768-dim)
- Reranker: qwen3-reranker-0.6B (~0.5GB)
- Total: ~6GB (leaves 10GB for system + Docker)
- See: specs/macbook-m2-setup.md for details

================================================================================
APIS TO CREATE
================================================================================

Document Management:
- POST   /api/rag/documents          (upload document)
- GET    /api/rag/documents          (list documents with filters)
- GET    /api/rag/documents/:id      (get document details)
- DELETE /api/rag/documents/:id      (delete document)
- GET    /api/rag/documents/:id/status (check processing status)

Search:
- POST   /api/rag/search             (search in documents)

RAG Chat:
- POST   /api/rag/chat               (chat with RAG context, streaming)

Configuration:
- GET    /api/rag/config             (get RAG settings)
- PUT    /api/rag/config             (update RAG settings)

Model Management:
- GET    /api/rag/models             (list available Ollama models)
- GET    /api/rag/models/profiles    (get model profile presets)
- POST   /api/rag/models/test        (test model configuration)
- PUT    /api/rag/models/active      (set active model profile)

Analytics:
- GET    /api/rag/analytics/queries  (query statistics)
- GET    /api/rag/analytics/documents (document statistics)

================================================================================
DATABASE CHANGES
================================================================================

NEW TABLES (PostgreSQL):

documents:
- id, filename, file_type, file_size, upload_date, processed_date
- status (pending, processing, completed, failed)
- metadata (category, tags, custom_fields as JSON)
- chunk_count, error_message

document_embeddings:
- document_id, embedding_model, embedding_dimensions
- indexed_at (track which model was used for indexing)

processing_jobs:
- id, document_id, status, priority, created_at, started_at, completed_at
- progress (0-100), retry_count, error_message

query_logs:
- id, query_text, conversation_id, timestamp, retrieval_time
- chunks_retrieved, response_time, user_rating (optional)

model_configurations:
- id, profile_id, profile_name, is_active
- llm_model, llm_provider, embedding_model, embedding_dimensions
- reranker_model, chunk_size, chunk_overlap, retrieval_top_k
- created_at, updated_at (store user's model configs)

VECTOR STORAGE (Weaviate):
- Document chunks with embeddings (dimension flexible based on model)
- Metadata (document_id, page_number, chunk_index, embedding_model)
- Support for hybrid search (vector + BM25)
- Multiple collections for different embedding dimensions if needed

================================================================================
DEPLOYMENT CHANGES
================================================================================

TARGET DEVELOPMENT ENVIRONMENT:
- MacBook with Apple M2 chip
- 16GB unified memory (shared CPU/GPU)
- macOS
- Docker Desktop for Mac

Docker Compose setup required for:
- Weaviate container (port 8080, max 2GB memory)
- PostgreSQL container (port 5432, max 1GB memory)
- Redis container (port 6379, max 512MB memory)
- Total Docker memory: ~3-4GB

All containers must:
- Run locally (no cloud)
- Have persistent volumes
- Be easily restartable
- Support development and production
- Have memory limits (for MacBook compatibility)

OLLAMA SETUP:
- Runs natively on macOS (not in Docker)
- Uses unified memory (~6GB for models)
- Loads models on-demand
- Supports Metal acceleration on M2

MEMORY BUDGET (MacBook M2 16GB):
- macOS System: ~3-4GB
- Docker Containers: ~3-4GB
- Ollama Models: ~6GB
- Browser/Apps: ~2-3GB
- Working Buffer: ~1-2GB
- Total: ~14-16GB (tight but workable)

SCALABILITY:
- System must work on MacBook M2 16GB
- System must scale to workstations with 64GB+ VRAM
- No code changes required when upgrading hardware
- Only model configuration changes needed

================================================================================
PERFORMANCE REQUIREMENTS
================================================================================

MACBOOK M2 16GB BASELINE (Development):
- Document processing: 10+ pages/second
- Retrieval latency: <200ms without reranking, <300ms with reranking
- End-to-end RAG response: <3 seconds (including LLM generation start)
- LLM generation: 20-30 tokens/second (8B model, Q4 quantization)
- Support 5,000-10,000 documents
- Support 2-5 concurrent queries
- Uptime: 95%+

WORKSTATION 64GB+ (Production):
- Document processing: 20+ pages/second
- Retrieval latency: <150ms without reranking, <250ms with reranking
- End-to-end RAG response: <2 seconds
- LLM generation: 50+ tokens/second (32B model)
- Support 100,000+ documents
- Support 10+ concurrent queries
- Uptime: 99%+

REQUIREMENTS ARE HARDWARE-DEPENDENT:
- System must gracefully handle memory constraints
- System must warn user if configuration exceeds available memory
- System must provide performance estimates based on hardware

================================================================================
SECURITY REQUIREMENTS
================================================================================

- All data stored locally (no cloud services)
- File upload restricted to PDF only
- Maximum file size enforced (50MB)
- SQL injection prevention for database queries
- API endpoints protected against malicious input
- No sensitive data in logs

================================================================================
SUCCESS METRICS
================================================================================

RETRIEVAL QUALITY:
- Precision@5 ≥ 80% (baseline on M2 16GB)
- Precision@5 ≥ 85% (target on better hardware)
- Mean Reciprocal Rank (MRR) ≥ 0.48
- User satisfaction: 80% of responses rated helpful

PERFORMANCE:
- Retrieval latency p95 < 300ms
- Document processing success rate ≥ 95%
- Zero data loss on system restarts
- System remains responsive on MacBook M2 16GB

USER EXPERIENCE:
- User can upload and query document within 2 minutes
- Citations are accurate 95%+ of the time
- System responds even when no relevant documents found
- User can switch models without technical knowledge
- System warns if configuration incompatible with hardware

FLEXIBILITY:
- User can upgrade from M2 16GB to workstation without code changes
- System handles model changes gracefully
- Re-indexing is automatic when embedding model changes

================================================================================
OUT OF SCOPE (FUTURE ENHANCEMENTS)
================================================================================

- GraphRAG (knowledge graph layer)
- Multi-user support with permissions
- Real-time collaboration
- Cloud deployment
- Mobile app
- Audio/video document processing
- Automatic document crawling/syncing
- Integration with external knowledge bases (Confluence, SharePoint)
- Advanced analytics and dashboards
- Cost optimization recommendations
- A/B testing framework
- Custom embedding model training

================================================================================
IMPORTANT DESIGN PRINCIPLES
================================================================================

MODEL AGNOSTIC ARCHITECTURE:
- NO hardcoded model names in application code
- Models are configured via settings, not code
- System must work with any Ollama-compatible model
- Users can switch models via UI without developer intervention
- Embedding dimension changes trigger automatic re-indexing

HARDWARE FLEXIBILITY:
- Primary development target: MacBook M2 16GB
- Must scale to high-end workstations (64GB+ VRAM)
- Memory limits enforced in Docker containers
- Performance requirements are hardware-dependent
- System warns if configuration exceeds available memory

UPGRADE PATH:
- User starts with 8B models on M2 16GB
- User upgrades hardware → switches to 32B/70B models
- No code changes required, only configuration
- System handles model migration automatically

REFERENCE DOCUMENTATION:
- specs/macbook-m2-setup.md - M2 16GB optimized configuration
- specs/model-recommendations-2026.md - Model benchmarks and recommendations
- specs/flexible-model-configuration.md - Model configuration system design
- specs/tech-stack-versions-2026.md - Current library versions and breaking changes
