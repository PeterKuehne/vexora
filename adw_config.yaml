project_name: Vexora
description: 'Implementing: Document Upload and Indexing, Hybrid Search (Vector + Keyword), Reranking for Better Accuracy
  (+6 more)'
existing_functionality:
- Conversations with Ollama-powered LLMs (Qwen and other models)
- Real-time streaming responses from LLM
- Multiple conversation management (create, switch, delete)
- Persistent conversation history in browser localStorage
- Ollama model selection and management
- Advanced LLM settings (temperature, top_p, top_k, tokens)
- Persistent system prompt configuration
- Light/Dark theme support
- Markdown rendering with code syntax highlighting
- Message editing and regeneration
- Auto-save functionality
- Settings import/export
requirements:
- name: Document Upload and Indexing
  priority: high
  libraries:
  - unpdf
  implementation_scope:
  - PDF document upload through UI
  - Document list view with processing status (pending, processing, completed, failed)
  - Document deletion functionality
  - Document filtering by category or tags
  - Metadata management (category, tags, custom fields)
  - Document chunking for searchable segments
  - Vector embedding conversion
  - Fast retrieval indexing
  - Metadata storage (filename, upload date, page numbers)
  constraints:
  - 'Maximum file size: 50MB per PDF'
  - Must handle multi-page PDFs
  - Processing must not block the UI
  - Users must see upload progress
  - Failed uploads must show clear error messages
  success_criteria:
  - User can upload a 20-page PDF and see it indexed within 30 seconds
  - User can see processing progress in real-time
  - User receives notification when processing completes or fails
  - Uploaded documents persist across browser sessions
  out_of_scope:
  - Word documents, Excel files (only PDF for MVP)
  - Batch upload of multiple files simultaneously
  - OCR for scanned PDFs
  - Document versioning
- name: Hybrid Search (Vector + Keyword)
  priority: high
  libraries:
  - weaviate-client
  implementation_scope:
  - Semantic search via vector similarity
  - Keyword search with exact/fuzzy matching
  - Combined scoring (hybrid approach)
  - User-configurable search behavior (semantic vs keyword balance)
  - Search filtering by document metadata
  - Display of documents used in response
  constraints:
  - Search latency must be under 300ms
  - Must support German and English documents
  - Must handle documents up to 1000 pages
  - Minimum 1000 documents supported
  success_criteria:
  - Finding 'Datenschutz DSGVO Artikel 6' returns relevant chunks from privacy policy
  - Searching for exact product codes works reliably
  - Users can understand why certain documents were retrieved
  out_of_scope:
  - Multi-language search across more than 2 languages
  - Image/diagram search within PDFs
  - Real-time document updates
- name: Reranking for Better Accuracy
  priority: medium
  libraries: []
  implementation_scope:
  - Re-scoring of candidate chunks for relevance
  - Cross-encoder model for accurate ranking via Ollama
  - Return only top 3-5 most relevant chunks
  - Toggle reranking on/off in settings
  constraints:
  - Reranking must complete within 100ms
  - Must work offline (local model)
  - Must support German and English
  success_criteria:
  - Retrieval precision improves by at least 15% with reranking enabled
  - Users can toggle reranking on/off in settings
  - Response quality is noticeably better with reranking
  out_of_scope:
  - Custom reranking model training
  - A/B testing framework
- name: RAG-Enhanced Chat
  priority: high
  libraries:
  - llamaindex
  implementation_scope:
  - Enable/disable RAG per conversation
  - Source citations in LLM responses
  - Clickable citations to view original document/page
  - Configurable chunk retrieval count (top-K)
  - Document filtering by category/tags for search
  - Retrieve relevant chunks before generating response
  - Include chunk context in LLM prompt
  - Stream responses with citations
  - Display contributing documents
  constraints:
  - RAG must not significantly slow down response time (max +500ms)
  - Citations must be accurate (correct document and page number)
  - RAG must work with existing streaming architecture
  - Must gracefully handle no relevant documents found
  success_criteria:
  - User asks 'What does our privacy policy say about GDPR Article 6?' and gets accurate answer with source citation
  - Citations link directly to the source document/page
  - 'Response indicates ''Based on 3 documents: privacy_policy.pdf (page 12), gdpr_guide.pdf (page 5)...'''
  - User can disable RAG and get normal chat behavior
  out_of_scope:
  - Multi-hop reasoning across documents
  - Automatic follow-up questions
  - Citation confidence scores
- name: Structured Data Queries
  priority: medium
  libraries: []
  implementation_scope:
  - Query structured data from PostgreSQL database
  - Combine document search with database queries
  - Merge answers from both sources
  - Store structured business data in PostgreSQL
  - Natural language to SQL conversion
  - Clear indication when database was queried
  constraints:
  - Read-only database access (no writes/updates)
  - Must work with existing PostgreSQL schemas
  - Queries must complete within 2 seconds
  - Must prevent SQL injection
  success_criteria:
  - User asks 'How many customers do we have in Berlin?' and gets answer from database
  - User asks 'What does the manual say about customer onboarding?' and gets answer from documents
  - System can handle questions requiring both sources
  out_of_scope:
  - Database schema modifications
  - Complex multi-table joins
  - Real-time database syncing
  - Write operations to database
- name: Document Management UI
  priority: high
  libraries: []
  implementation_scope:
  - Dashboard to view all uploaded documents
  - Processing status and error display
  - Document search and filtering
  - Document deletion
  - Document metadata preview
  - Bulk delete multiple documents
  constraints:
  - Must be responsive (mobile-friendly)
  - Must show real-time processing updates
  - Must handle 1000+ documents without performance issues
  - Must integrate seamlessly with existing UI theme
  success_criteria:
  - User can find any document within 3 clicks
  - User can see why a document failed to process
  - User can delete multiple documents at once
  - Dashboard loads in under 1 second even with 1000 documents
  out_of_scope:
  - Document preview/reader
  - Collaborative document sharing
  - Document annotations
- name: Model Configuration & Flexibility
  priority: high
  libraries: []
  implementation_scope:
  - Select LLM model from available Ollama models
  - Select embedding model from available Ollama models
  - Select reranker model from available Ollama models
  - Switch between model profiles (e.g., 'MacBook 16GB', 'Workstation 64GB')
  - Create custom model configurations
  - View current model memory usage
  - Test model configuration before applying
  - Auto-discover available Ollama models
  - Detect embedding dimension changes
  - Offer to re-index documents when embedding model changes
  - Show model compatibility warnings
  - Support profile presets for different hardware configurations
  constraints:
  - Must not hardcode model names in application code
  - Must handle different embedding dimensions (768, 1024, 8192, etc.)
  - Must work on MacBook M2 16GB unified memory
  - Must support future hardware upgrades without code changes
  - Settings must persist across sessions
  success_criteria:
  - User can switch from 8B to 32B LLM model without code changes
  - User can upgrade embedding model and system handles re-indexing
  - System shows 'Model incompatible with 16GB memory' warning appropriately
  - Profile switching works seamlessly (select preset -> apply -> done)
  - New models pulled via Ollama automatically appear in UI
  out_of_scope:
  - Cloud API providers (OpenAI, Anthropic) in MVP
  - Custom model training
  - Automatic model selection based on query complexity
- name: RAG Configuration & Settings
  priority: medium
  libraries: []
  implementation_scope:
  - 'Hybrid search balance configuration (semantic vs keyword weight: alpha parameter)'
  - 'Number of chunks to retrieve (top-K: 1-10)'
  - Enable/disable reranking toggle
  - Default categories/tags configuration
  - Chunk size and overlap settings (advanced)
  constraints:
  - Settings must persist across sessions
  - Changes must apply immediately
  - Must provide sensible defaults
  - Must include helpful tooltips
  success_criteria:
  - User can adjust alpha from 0.0 (pure keyword) to 1.0 (pure semantic)
  - User can increase top-K to 10 for more context
  - Settings UI is intuitive for non-technical users
  out_of_scope:
  - A/B testing of different configurations
  - Automatic parameter tuning
- name: Analytics & Monitoring
  priority: low
  libraries: []
  implementation_scope:
  - Display total number of documents indexed
  - Storage usage display
  - Most frequently queried documents
  - Average retrieval time
  - Query success rate
  constraints:
  - Analytics must not impact performance
  - Data stored locally (no external analytics)
  - Basic visualizations only
  success_criteria:
  - User can see total documents and storage used
  - User can identify which documents are never used
  - System logs errors for debugging
  out_of_scope:
  - Advanced analytics dashboard
  - Query optimization recommendations
  - Cost tracking
tech_stack:
  new_dependencies:
    npm:
    - unpdf
    - llamaindex
    - weaviate-client
    pip: []
    docker:
    - weaviate/weaviate:1.34.8
    - postgres:16
    - redis:7
  apis_to_create:
  - endpoint: POST /api/rag/documents
    description: Upload document
    authentication: None (single-user local)
  - endpoint: GET /api/rag/documents
    description: List documents with filters
    authentication: None (single-user local)
  - endpoint: GET /api/rag/documents/:id
    description: Get document details
    authentication: None (single-user local)
  - endpoint: DELETE /api/rag/documents/:id
    description: Delete document
    authentication: None (single-user local)
  - endpoint: GET /api/rag/documents/:id/status
    description: Check processing status
    authentication: None (single-user local)
  - endpoint: POST /api/rag/search
    description: Search in documents (hybrid search)
    authentication: None (single-user local)
  - endpoint: POST /api/rag/chat
    description: Chat with RAG context, streaming
    authentication: None (single-user local)
  - endpoint: GET /api/rag/config
    description: Get RAG settings
    authentication: None (single-user local)
  - endpoint: PUT /api/rag/config
    description: Update RAG settings
    authentication: None (single-user local)
  - endpoint: GET /api/rag/models
    description: List available Ollama models
    authentication: None (single-user local)
  - endpoint: GET /api/rag/models/profiles
    description: Get model profile presets
    authentication: None (single-user local)
  - endpoint: POST /api/rag/models/test
    description: Test model configuration
    authentication: None (single-user local)
  - endpoint: PUT /api/rag/models/active
    description: Set active model profile
    authentication: None (single-user local)
  - endpoint: GET /api/rag/analytics/queries
    description: Query statistics
    authentication: None (single-user local)
  - endpoint: GET /api/rag/analytics/documents
    description: Document statistics
    authentication: None (single-user local)
  database_changes:
  - Add documents table (id, filename, file_type, file_size, upload_date, processed_date, status, metadata, chunk_count, error_message)
  - Add document_embeddings table (document_id, embedding_model, embedding_dimensions, indexed_at)
  - Add processing_jobs table (id, document_id, status, priority, created_at, started_at, completed_at, progress, retry_count,
    error_message)
  - Add query_logs table (id, query_text, conversation_id, timestamp, retrieval_time, chunks_retrieved, response_time, user_rating)
  - Add model_configurations table (id, profile_id, profile_name, is_active, llm_model, llm_provider, embedding_model, embedding_dimensions,
    reranker_model, chunk_size, chunk_overlap, retrieval_top_k, created_at, updated_at)
  - 'Weaviate: Document chunks collection with flexible dimension embeddings'
  - 'Weaviate: Metadata (document_id, page_number, chunk_index, embedding_model)'
  - 'Weaviate: Multiple collections for different embedding dimensions if needed'
  - Install pgvector 0.8.1 extension for PostgreSQL
  infrastructure:
    docker_services:
    - name: weaviate
      image: weaviate/weaviate:1.34.8+
      port: 8080
      memory_limit: 2GB
      description: Vector database with native hybrid search
    - name: postgresql
      image: postgres:16+
      port: 5432
      memory_limit: 1GB
      description: Structured data storage with pgvector extension
    - name: redis
      image: redis:7.x
      port: 6379
      memory_limit: 512MB
      description: Caching and background job processing
    ollama_models:
      recommended_starter:
        llm: qwen3:8b-q4_K_M (~5GB)
        embedding: nomic-embed-text (~0.5GB, 768-dim)
        reranker: qwen3-reranker-0.6B (~0.5GB)
        total_memory: ~6GB
      note: Models are user-configurable via settings, not hardcoded
apps:
- name: frontend
  path: .
  type: frontend
  default_port: 5173
conventions:
  version: 5
  created_at: '2026-01-20T11:37:51.936316Z'
  source: agent-discovered
  styling:
    system: tailwind
    discovered_in_feature: 2
    discovered_at: '2026-01-20T11:37:51.936541Z'
    import_path: tailwindcss
    reference_files:
    - src/components/DocumentUpload.tsx:13
    - src/components/DocumentList.tsx:30
    - src/components/SidebarTabs.tsx:25
    usage_pattern: 'Theme-aware conditional classes: ${isDark ? ''bg-gray-800 text-white'' : ''bg-white text-gray-900''}"'
    guidelines:
    - Always use isDark conditional styling
    - Use semantic color names (bg-surface, text-primary)
    - Apply hover states consistently
    - Use transition-all for smooth animations
  last_updated: '2026-01-20T21:08:51.042830Z'
  updated_by_feature: 3
  api:
    system: express_rest
    discovered_in_feature: 2
    discovered_at: '2026-01-20T11:37:58.225759Z'
    import_path: express
    reference_files:
    - server/src/index.ts:258
    - src/lib/api.ts:383
    usage_pattern: REST endpoints with asyncHandler, FormData for uploads, Zod validation, proper HTTP status codes"
    guidelines:
    - Use asyncHandler for all async routes
    - Validate with Zod schemas
    - Use multer for file uploads
    - Return consistent JSON responses
    - Use proper HTTP status codes (201 for creation, 404 for not found)
  state_management:
    system: react_context
    discovered_in_feature: 2
    discovered_at: '2026-01-20T11:38:04.589302Z'
    import_path: react
    reference_files:
    - src/contexts/DocumentContext.tsx:45
    - src/contexts/DocumentContext.tsx:163
    usage_pattern: React Context + Provider pattern with custom hooks, useCallback for performance"
    guidelines:
    - Create Context + Provider + Hook trio
    - Use useCallback for async actions
    - Include loading/error states
    - Export types for TypeScript
    - Use custom hooks like useDocuments()
  async_processing:
    system: processing_job_service
    discovered_in_feature: 3
    discovered_at: '2026-01-20T21:08:51.042564Z'
    import_path: server/src/services/ProcessingJobService.ts
    reference_files:
    - server/src/services/ProcessingJobService.ts:1
    - server/src/index.ts:353
    - src/hooks/useProcessing.ts:1
    usage_pattern: 'Event-based Job Queue: processingJobService.createJob() → Real-time Socket.io Updates → Frontend useProcessing()
      Hook'
    guidelines:
    - Use EventEmitter for real-time updates
    - Broadcast via Socket.io to all clients
    - Track progress with currentChunk/totalChunks
    - Frontend hooks for state management
    - 'Status: pending → processing → completed/failed'
